# 2025-10-18 - Saturday

## ğŸŒ… Morning Routine
- [ ] Wake up at 6:00 AM
- [ ] Brush teeth by 6:15 AM
- [ ] Drink water
- [ ] Shambhavi Mahamudra (40 min)
- [ ] Prepare food
- [ ] Leave for work by 8:00 AM

## ğŸ¯ Today's Top 3
1. [ ] Diwali Cleaning
2. [ ] PhD - set up training for multi-input to the vit (fmf type) (1.1).
3. [ ] Mobai - review the documentation generated yesterday

## ğŸ’¼ Work Log (Mobai)
**Standup:**
- Yesterday: 
- Today: 
- Blockers: 

**Progress:**
- 

**Notes:**
- 

## ğŸ”¬ PhD Work (8-10 PM)
**Focus:** 
1. For the same vit backbone and then adding attention to the stacked embeddings? kind of like the fmf-pad where instead of different modalities we use different filtered images?Â  Inputting:
    1. low pass and high pass filtered images
    2. just the fourier transform/wavelet transform images
2. Use HRM type 2 transformer blocks (its initialisation would be from existing clip or dino blocks, or just random weights),
    1. where we perform attention on all the nodes or half of the nodes so there would be 2 streams. Main idea is to have a finegrained and coarse focus on the image.Â  Â 
    2. applying hrm directly.. assume small number of transfomer blocks like 1 to 3 can do make up the high level and low level module and just make it iterate for small depth values maybe effective depth of 12 (standard vit) to 18 and then use that
3. What if we do the shaprness aware minimization for per domain just like gacd but we adaptively to the minimisation of the crossentropy and contrastive loss? this would basically help us to minimise the predictionâ€™s loss and maximise the clustering of each domains.

**Progress:**
- 

**Papers Read:**
- 

**Concepts Learned:**
- 

## ğŸƒ Wellness
- [ ] Gym (30 min) - Returned by 3:15 PM
- [ ] Shambhavi Mahamudra evening (25 min)
- [ ] NoFap day âœ“
- [ ] Quality time with wife (dinner & stroll)
- [ ] Quality time in bed

## ğŸŒŸ Daily Reflection
**What went well:**


**What to improve:**


**Grateful for:**


**Tomorrow's priority:**


## ğŸ”— Linked Notes
- 

---
Tags: #daily #2025 #2025-10
