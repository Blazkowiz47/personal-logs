---
aliases: ["Multi-Teacher Single-Student Visual Transformer with Multi-Level Attention for Face Spoofing Detection"]
tags: [paper, pad, deep-fas-survey, binary-supervision, visual-transformer, multi-teacher]
authors: Yaojia Huang, Jun-Wei Hsieh, Ming-Ching Chang, Lipeng Ke, Siwei Lyu, Avik Santra
year: 2021
venue: BMVC
paper_url: https://cse.buffalo.edu/~siweilyu/papers/BMVC2021.pdf
code_url: 
status: "âœ… Read"
dateadded: 2025-11-26
dateread: 2025-11-26
priority: medium
---

## Quick Summary
**Method:** MTSS
- **Backbone:** ViT+Multi-Level Attention Module
- **Loss:** CE Loss
- **Input:** RGB
- **Static/Dynamic:** S

## Abstract
A new Multi-Teacher Single-Student (MTSS) visual Transformer with a multi-level attention design is proposed with a novel Multi-Level Attention Module with a DropBlock (MAMD) designed to strengthen discriminative features while dropping irrelevant spatial features to avoid overfitting.
---

## Quick Summary
**Method:** MTSS
- **Backbone:** ViT+Multi-Level Attention Module
- **Loss:** CE Loss
- **Input:** RGB
- **Static/Dynamic:** S

## Abstract
A new Multi-Teacher Single-Student (MTSS) visual Transformer with a multi-level attention design is proposed with a novel Multi-Level Attention Module with a DropBlock (MAMD) designed to strengthen discriminative features while dropping irrelevant spatial features to avoid overfitting.



## Problem Statement
What problem does this paper address?


## Key Contributions
1. 
2. 
3. 

## Methodology
### Architecture
*Describe the model/approach*


### Key Techniques
- 
- 

### Novel Components
*What's new/different from prior work?*


## Experiments
### Datasets Used
- 
- 

### Results
*Key metrics and performance*

| Dataset | Metric | Result | Baseline |
|---------|--------|--------|----------|
|         |        |        |          |

### Ablation Studies
*What components were tested?*


## Strengths
- 
- 

## Limitations
- 
- 

## Critical Analysis
*My thoughts on the paper*

### What Works Well
- 

### Concerns/Criticisms
- 

### Missing Pieces
- 

## Relevance to My Work
*How does this relate to my PAD research?*

### Direct Applications
- 

### Ideas Sparked
- 

### Techniques to Borrow
- 

## Implementation Notes
*Anything useful for implementing this*

### Architecture Details
- 

### Hyperparameters
- 

### Training Details
- 

### Reproducibility Notes
- 

## Related Papers
### Cited By This Paper
- [[]]

### Papers That Cite This
- [[]]

### Similar Approaches
- [[]]

## Questions & Future Directions
### Open Questions
- 

### Extension Ideas
- 

### Experimental Ideas
- 

## Notes & Highlights
### Key Quotes
> 

### Figures to Remember
- Figure X: 

### Equations
$$
$$

## Meeting Notes
*Discussions with advisor/colleagues about this paper*


## Action Items
- [ ] 
- [ ] 

---
**Reading Progress:** 
- [ ] Abstract
- [ ] Introduction
- [ ] Related Work
- [ ] Methodology
- [ ] Experiments
- [ ] Conclusion
- [ ] Supplementary Material
