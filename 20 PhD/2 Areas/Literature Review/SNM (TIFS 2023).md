---
aliases: [snm_fas]
tags: [paper, pad, deep-fas-survey, multimodal]
authors: Pengchao Deng, Chenyang Ge, Xin Qiao, Hao Wei, Yuan Sun
year: 2023
venue: IEEE Transactions on Information Forensics and Security (TIFS)
paper_url: https://ieeexplore.ieee.org/abstract/document/10176121
code_url: 
status: "ðŸ“š To Read"
dateadded: 2025-11-26
dateread: 
priority: medium
---
# Attention-Aware Dual-Stream Network for Multimodal Face Anti-Spoofing

> [!abstract]
> As a closely related topic, multimodal face anti-spoofing (FAS) has become an indispensable part of face recognition systems. However, existing multimodal FAS tools suffer from performance degradation under external low-lighting conditions and the lack of effective fusion strategies. To address these issues, we propose an Attention-Aware Dual-Stream Network (ADSN) for multimodal FAS. Specifically, we first design a dual-stream network to extract features from RGB and IR/Depth modalities. Then, we introduce an attention mechanism to adaptively fuse the features from different modalities. Finally, we conduct extensive experiments on three public datasets to demonstrate the effectiveness of the proposed method.

## What does the paper present?
- **Backbone:** ResNet18
- **Loss:** BCE Loss, center loss, cosine loss
- **Input:** Depth, IR
- **Fusion:** Feature-level

What problem does this paper address?

*Describe the model/approach*

*What's new/different from prior work?*

*Key metrics and performance*

*What components were tested?*

*Anything useful for implementing this*

- [[]]

- [[]]

- [[]]

## What are my views on it?
*My thoughts on the paper*

*How does this relate to my PAD research?*

>

- Figure X:

$$
$$

*Discussions with advisor/colleagues about this paper*

- [ ]
- [ ]

---
**Reading Progress:**
- [ ] Abstract
- [ ] Introduction
- [ ] Related Work
- [ ] Methodology
- [ ] Experiments
- [ ] Conclusion
- [ ] Supplementary Material
